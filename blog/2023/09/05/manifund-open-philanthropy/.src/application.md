OpenPhil Grant Application
==========================

- Created: August 4, 2023 8:06 PM
- Status: submitted

## Proposal Summary (<20 words)

Manifund: longtermist grantmaker experimenting with regranting, impact certs and other funding mechanisms.

## Project Description (<750 words)

Manifund started in Jan 2023 by building a website for impact certs for the [ACX Mini-Grants round](https://manifund.org/rounds/acx-mini-grants); we then hosted impact certs for the [OpenPhil AI Worldviews contest](https://manifund.org/rounds/ai-worldviews). Since May, we’ve been focused on [a regranting program](https://manifund.org/rounds/regrants). We’d like to scale up our regranting program, and experiment with other funding mechanisms and initiatives.

Some things that distinguish us from other longtermist funders:

- We’re steeped in the tech startup mindset: we move quickly, ship often, automate our workflows, and talk to our users.
- We’re really into transparency: all projects, writeups, and grant amounts are posted publicly on our site. We think this improves trust in the funding ecosystem, helps grantees understand what funders are interested in, and allows newer grantmakers to develop a legible track record.
- We care deeply about grantee experience: we’ve spent a lot of time on the other side of the table applying for grants, and are familiar with common pain points: long/unclear timelines, lack of funder feedback, confusing processes.
- We decentralize where it makes sense: regranting and impact certs are both funding models that play to the strengths of networks of experts. Central grantmaker time has been a bottleneck on nurturing impactful projects; we hope to fix this.

Overall, we hope to help OpenPhil decentralize funding to other decisionmakers in a way that rewards transparent, accountable, timely and cost-effective grants.

### Scaling up regranting

Regranting is a system where individuals are given small discretionary budgets to make grants from. This allow regrantors to find projects in their personal and professional networks, seed new projects based on their interests, and commit to grantees with little friction. Our current regrantors are drawn from Anthropic, OpenAI, Rethink Priorities, ARC Evals, CAIS, FAR AI, SERI MATS, 1DaySooner and others; see all regrantors [here](https://manifund.org/rounds/regrants?tab=regrants), and a recent payout report [here](https://manifund.substack.com/p/what-were-funding-weeks-2-4).

With further funding, we’d like to:

   Onboard new regrantors: we currently have a waitlist of qualified regrantors and get about one new strong application per week. We think we could find many more promising candidates over the next year, to sponsor with budgets of $50k to $400k.
   Increase budgets of high-performing regrantors: this incentivizes regrantors to make good grants, delegates authority to people who have performed well in the past, and quantifies their track record.
   Hire another team member: Our team is currently just 2 people, both of us are wearing a lot of hats, and one (Austin) spends ~30% of his time on Manifold. We’re interested in finding a fulltime in-house grantmaker for grant evaluation, regrantor assessment, and public comms. Ideally, this person would take on a cofounder-like role and weigh in on Manifund strategy. Later, we may want to hire another engineer or ops role.

### Running a larger test of impact certs

Impact certs are a new mechanism for funding nonprofits. A philanthropic donor announces a large prize; projects compete for the prize and raise funding by selling “impact certs” (shares of prize winnings) to self-interested investors. This allows donors to pay directly for impact, while investors take on risk for return.

In the spring, we ran [a trial with Scott Alexander](https://manifund.org/rounds/acx-mini-grants) with a $40k prize pool; this round will conclude this September. Scott is still considering whether to do Round 2 of ACX Grants round through impact certs; if so, we’d host that this fall. We also tried an impact cert round with the [OpenPhil AI Worldviews Contest](https://manifund.org/rounds/ai-worldviews), with somewhat less total engagement.

Our most ambitious goal would be a AI Safety impact cert round with yearly, large (>$1m) prize pool. With less funding, we might experiment with an e.g. EA Art prize to test the waters.

Other possible initiatives & funding experiments

- Setting up a Common App for longtermist funding. We’re coordinating with LTFF on this, and would love to include funders like OpenPhil, Lightspeed, SFF and independent donors. This could alleviate a key pain point for grantees (each app takes a long time to write!) and help funding orgs find apps in their area of interest.
- Start “EA peer bonuses”, inspired by the Google peer bonus program. We’d let anyone nominate community members who have done valuable work, and pay out small (~$100) prizes.
- Run a 1-week “instant grants” program, where we make a really short application (eg 3 sentences and a link) for grants of $1000 and get back to applicants within 24 hours.
- Paying feedback prizes to users & regrantors for making helpful comments on projects, both to inform funding decisions and to give feedback to grantees.

## Why we’re a good fit for this project (300 words)

We have the right background to pursue philanthropic experiments:

- Austin founded Manifold, which involves many of the same skills as making Manifund go well: designing a good product, managing people and setting culture, iterating and knowing when to switch directions. Also, Manifold ventures outside the boundaries of the EA community (eg having been cited by Paul Graham and the NYT podcast Hard Fork); Manifund is likewise interested in expanding beyond EA, especially among its donor base. Austin is in a good position to make this happen, as he’s spent most of his professional career in non-EA tech companies (Google) & startups (tech lead at Streamlit), has an impressive background by their lights, and shares many of their sensibilities.
- Rachel is early in her career, but has a strong understanding of EA ideas and community, having founded EA Tufts and worked on lots of EA events including EAGx Berkeley, Future Forum, Harvard and MIT AI safety retreats, and GCP workshops. She started web dev 8 months ago, but has gotten top-tier mentorship from Austin, and most importantly, built almost the entire Manifund site herself, which people have been impressed by the design and usability of.

## Approximate budget

We’re seeking $5 million in unrestricted funding from OpenPhil. Our intended breakdown:

- $3m: Regrantor budgets. Raise budgets of current regrantors every few months according to prior performance; onboard new regrantors.
  - We currently have 15 regrantors with an average budget $120k; we’d like to sponsor 25 regrantors with an average budget $200k.
  - Our existing regrantors already have a large wishlist of projects they think they can allocate eg above the current LTFF bar; we can provide examples on request.
- $1.5m: Impact certs and other funding experiments, as listed in project description.
  - Our regranting program itself is an example of something we funded out of our discretionary experimental budget.
- $0.5m: General operational costs, including salaries for a team of 2-3 FTE, software, cloud costs, legal consultations, events we may want to run, etc.

In total, Manifund has raised ~$2.4m since inception from an anonymous donor ($1.5m), Future Fund ($0.5m) & SFF ($0.4m) and committed ~$0.8m of it. We intend to further fundraise up to a budget of $10m for the next year, seeking funding from groups such as SFF/Jaan Tallinn, Schmidt Ventures, YCombinator, and small to medium individual donors.

_Thanks to Joel, Gavriel, Marcus & Renan for feedback on this application._

## Appendix

### Cut

- This allows us to structure the site like a forum, with comments and votes. One vision of what we could be is “like the EA forum but oriented around projects”.
- We aim to model transparency ourselves, with our code, finances, and vast majority of meeting notes and internal docs visible to the public.
- Our general evaluation model is that regrantors screen for effectiveness, and Manifund screens for legality and safety, but in the case of projects with COIs or with a large portion of funding coming directly from donors, we should really be screening for effectiveness and would like to have someone on board who’s better suited to do that

### Draft notes

- in the past did impact certs for ACX and OP AI Worldviews
- currently focused on regranting
- generally: would be good if EA funding were more diverse. Also faster, more transparent, possibly with better incentives.
- interested in scaling up regranting program:
  - offer more budgets, and raise budgets according to past performance (S-process or something)
  - looking for funding for regrantors from other sources, particularly interested in getting outside-of-EA funding, think we’re better suited to do that than e.g. LTFF because of Austin’s background and our branding, maybe OpenPhil could just cover ops.
  - kind of want to hire another person, maybe someone with more grantmaking experience who can act like a reviewer and help us with strategy. Useful especially in cases where a regrantor wants to give to something with a COI or where a large portion of funds are coming from random users/donors instead of regrantors and we want to evaluate grants for real. Currently only ~1.75 people on our team.
- and doing more with impact certs: possibly will host Scott’s next ACX round, ultimately interested in something like yearly big prize for AI safety where projects are initially funded through impact certs, and we might do medium things on the way to test whether this is a good idea and refine our approach.
- other experiments we’re considering:
  - CommonApp with LTFF, maybe include lightspeed or other funders
  - generally work on schemes for better coordination between funders
  - start EA peer bonus program [EA peer bonus](https://www.notion.so/EA-peer-bonus-2f268e716a5e4f81acb3e9f642f6842f?pvs=21)
  - do ~week long “instant” grants program [Instant grants](https://www.notion.so/Instant-grants-bf88f0b2ecb142fd890c462fad115037?pvs=21)
  - paying users/regrantors retroactively for making really helpful comments on projects
- some things we’re doing differently from other funders that we think are promising:
  - being really transparent: some people said in advance and we worried that this would be too limiting, but regrantors haven’t really complained about it so far. And we think it has a bunch of positive externalities: shows people what types of projects grantmakers are interested in funding and why, generates hype/publicity for cool projects (e.g. cavities, shrimp welfare), allows regrantors to generate a public trackrecord, generates trust
  - relatedly, generally using software: makes it easy to be transparent, also makes it easy to facilitate conversations between grantees and grantmakers and other community members. Makes things faster and smoother, allows us to set defaults. Looks different, might appeal to e.g. rich tech ppl more.
    - general attitude with Austin’s background also may be different/advantageous: move fast, do lots of user interviews and generally focus a lot on user experience
  - giving small budgets to somewhat less well-known people so they can build up a track record

- some things we’re doing less well than other funders:
  - generally being really careful/optimizing really hard about where the money goes or something. We heavily outsource donation decisions to regrantors, and ultimately just screen for legality/non-harmfulness. An analogy we use a lot is to the FDA’s approval process: Manifund covers safety, regrantors cover efficacy. Currently there aren’t that many regrantors and just having them in a discord channel together facilitates lots of the good/necessary coordination, so not that worried about unilateralists curse rn, but could be a problem later. We aren’t screening $50k regrantors that rigorously in advance: we take applications, do an interview, ask the community health team, talk about it…but ultimately we’re pretty down to take bets. This means we probably want to up budgets more carefully.
  - some ops stuff. Don’t have a good way of sending money internationally.
