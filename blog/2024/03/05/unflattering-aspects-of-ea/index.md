Unflattering aspects of Effective Altruism
==========================================

<h3>1. I want to better understand in order to better decide</h3>

<p>As a counterbalance to the rosier and more philosophical perspective that Effective Altruism (EA) likes to present of <a href="https://forum.effectivealtruism.org/handbook">itself</a>, I describe some unflattering aspects of EA. These are based on my own experiences with it, and my own disillusionments<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>.</p>

<p>If people getting into EA<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> have a better idea of what they are getting into, and decide to continue, maybe they&rsquo;ll think twice, interact with EA less naïvely and more strategically, and not become as disillusioned as I have.</p>

<p>But also, the EA machine has been making some weird and mediocre moves, leaving EA as a whole as a not very formidable army<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>. A leitmotiv from the Spanish epic poem <a href="https://wikiless.nunosempere.com/wiki/Cantar_de_mio_Cid?lang=en"><em>The Song of the Cid</em></a> is &ldquo;God, what a good knight would the Cid be, if only he had a good lord to serve under&rdquo;. As in the story of the Cid then, so in EA now. As a result, I think it makes sense for the rank and file EAs to more often do something different from EA™, from EA-the-meme. To notice that taking EA money carries costs. To reflect on whether the EA machine is better than their outside options. To walk away more often.</p>

<h3>2. That the structural organization of the movement is distinct from the philosophy</h3>

<p>Effective altruism&rsquo;s philosophical ideas are seductive: who wants to be less effective? who wants to work on intractable, overgrazed and worthless projects, as opposed to tractable, neglected and impactful problems? But liking the philosophy doesn&rsquo;t mean you will like the actual movement, or that you should join it. You can have many different kinds of organizational structures corresponding to the same philosophy, and some will be a poor fit for you.</p>

<p>For example, after the 2008 crisis, one could be in favor of reforming the US financial system and holding those responsible for the 2008 crisis accountable, but find <em>Occupy Wall Street</em> deeply disappointing. Historically, there has been huge confusion about this point in EA.<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<h3>3. and EA structurally orients itself around one billionaire&rsquo;s money.</h3>

<p>To a first approximation, the structural organization of Effective Altruism is as follows:</p>

<p><img src="https://images.nunosempere.com/blog/2023/11/08/unflattering-aspects-of-ea/ea-organizational-structure.png" alt="" /></p>

<ul>
<li>Dustin Moskovitz, a deca-billionaire, is giving his fortune away through his foundation. His foundation, Open Philanthropy, has a large staff subdivided into cause areas.</li>
<li>Organizations are chasing Open Philanthropy&rsquo;s funding.</li>
<li>Rank and file members are seeking to work at organizations with Open Philanthropy funding (&ldquo;EA organizations&rdquo;)</li>
</ul>


<p>There are players who do not fit into this scheme, but I would describe their contribution as marginal. Not as irrelevant, mind you, just as very small in comparison with the Open Philanthropy juggernaut. Still, a few points of nuance:</p>

<ul>
<li>Dustin Moskovitz (ca. $10B) isn&rsquo;t the only billionaire giving money to the cluster of organizations under the EA banner. There is also Jaan Tallinn (ca. $1B), which gives under various &ldquo;Survival and Flourishing&rdquo; funds. More may be coming.</li>
<li>There are a few people &ldquo;earning to give&rdquo;, or donating independently of Open Philanthropy. The ones I know of are smaller, with a net worth of ca. ~$10M or so.</li>
<li>Not 100% organizations or individuals in the EA movement are chasing Open Philanthropy funding.</li>
<li>Sometimes, Open Philanthropy doesn&rsquo;t donate to projects directly but e.g., donates to some <a href="https://funds.effectivealtruism.org/">Effective Altruism Fund</a> or to the Centre for Effective Altruism, which donates to the final project.</li>
<li>etc.<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup></li>
</ul>


<p>Still, the decisions of Open Philanthropy end up being decisive. How decisive? Well, Open Philanthropy directs something like 90% of current funding within the EA movement<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>. So other funders just don&rsquo;t have as much capacity in comparison. For example, running a 10 person organization in the EA movement really benefits from having backing from Open Philanthropy, because relying on the other funders adds too much uncertainty and volatility. So I&rsquo;d say that they end up being pretty decisive.</p>

<h3>4. In practice, cost-effectiveness estimates keep EA honest, but only for global health</h3>

<p>If we have some reliable way of estimating the value of projects, structural organization doesn&rsquo;t matter that much. You would propose your project, it would be evaluated, and if it was above some cost-effectiveness bar, it would be funded. That is, to a first approximation, what happens within the global health cause area in EA. You can seek to objectively<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> estimate the quality-adjusted life years that an intervention saves. You can have an evaluator like GiveWell. And you can have an organization like Charity Entrepreneurship trying to find interventions that would be evaluated favorably by GiveWell.</p>

<p>The situation with animal welfare is a bit messier. Open Philanthropy might be making some quantified estimates, but I don&rsquo;t recall them being public. And Animal Charity Evaluators, the would-be GiveWell equivalent, doesn&rsquo;t do quantified estimates of the value of the charities they rank. Still, in principle you <em>could</em> do estimates of value for animal suffering interventions and avoid the problems I outline below.</p>

<p>With longtermism and global catastrophic risks, you don&rsquo;t have good methods of estimating the value of different interventions, for example of determining that one AI safety research agenda is better than another, or that one AI governance approach is superior. So in practice, you end up relying on the personal judgment of a crowd of amalgamated<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup> EA leaders for making funding and prioritization decisions.</p>

<p>Historically, Open Philanthropy has been slow to trust people, either as employees or as grantees. So these amalgamated EA leaders have been overworked, busy, unapproachable<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup>. In practice, people go to great lengths to try to approach and socialize with Open Philanthropy employees, like visiting or moving to the very expensive San Francisco Bay area.</p>

<p>That grant-makers are busy and unavailable makes getting access to them hard, because the group has limited available throughput. But say you increase the throughput. Then, if the game and the habits are still to compete for a limited pool of resources, and if there is still infinite demand for free billionaire money, then charismatic grantees close to EA leaders will still out-compete others. Competing for access is still the wrong game to be playing, though, and I resent this; you don&rsquo;t want to have a pool of talent competing hard for grant-maker attention, you want to have a pool of talent working hard at making the world a better place<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup>.</p>

<p>
<img src="https://images.nunosempere.com/blog/2023/11/08/unflattering-aspects-of-ea/watercolor-small.png" style="max-width: 50%" alt=""/>
<figcaption>The sun provides a source of energy; the sunflower evolves to follow it. So with Open Philanthropy and Effective Altruism. I&rsquo;m then saying that in a sunflower field, flowers who don&rsquo;t move to track the sun could be out-competed. But tracking the sun is a distraction, an instrumental goal at best.</figcaption></p>

<p>The same story told from the bottom up is: an aspiring EA starts with the intention of doing large amounts of good, and will try to do something semi-ambitious. Then he&rsquo;ll find out that funding constraints are a big part of making shit happen. And when solving that funding bottleneck, he will be in a social context where the natural good move is to try to get access and then seduce a busy, overworked, and therefore unavailable coterie of grantmakers<sup id="fnref:11"><a href="#fn:11" rel="footnote">11</a></sup><sup>,</sup><sup id="fnref:12"><a href="#fn:12" rel="footnote">12</a></sup>. He&rsquo;ll burn out.</p>

<p>But that&rsquo;s the wrong game to be playing because if you look at autochthonous EAs, at the rank and file, many are nerds, nerds who are able to do good work but who will find it hard to jockey for access. Their winning move would be not to play, and to gain real power by building something independently.</p>

<h3>5. Outside of global health, the leadership of the EA machinery has even more unappealing aspects</h3>

<p>Even beyond the sunflower issues, the central EA machinery, at organizations like the Center for Effective Altruism or Open Philanthropy, has other issues that make it unappealing to me as a source of leadership—of guidance, of evaluation, of moral direction:</p>

<p>First, their priorities are different from mine: Open Philanthropy seems fairly committed to <a href="https://nunosempere.com/blog/2023/04/25/worldview-diversification/">worldview diversification</a>, which I consider a mediocre framework. The Center for Effective Altruism cares much more about the reputation of the &ldquo;Effective Altruism&rdquo; brand than I do. In general, I get the impression that they want to &ldquo;be in control&rdquo;, and reduce variance from people they don&rsquo;t deeply trust, while at the same time coming to trust people slowly. In contrast, I would prefer to increase <a href="https://nunosempere.com/blog/2023/07/19/better-harder-faster-stronger/">formidability</a>, to employ <a href="https://nunosempere.com/blog/2023/11/30/auftragstaktik/">Auftragstaktik</a>.</p>

<p>As a small but very concrete example of the disconnect between my priorities and those of the EA machine, the EA forum has become a <a href="https://twitter.com/NunoSempere/status/1589968170953363456">worse place for me</a> over the last couple of years; it seems slower, more pushy, more censorious, more paternalistic. It started as a mean lean machine hosting community discussion, and it is now more of a vehicle for pushing ideas CEA wants you to know about. In the process it grew to cost $2M/year (!?!), employ six to eight people. You can see this thought elaborated further <a href="https://nunosempere.com/blog/2023/10/15/ea-forum-stewardship/">here</a>.</p>

<p>Second, I don&rsquo;t really understand how feedback loops work in Effective Altruism. If someone thinks that Open Philanthropy is making some mistakes, do they ¿write an EA Forum post and hope to get the attention of someone on inside an inner circle? ¿ambush someone at a party? ¿how do they find the party? ¿how do they get heard? Over the past years I&rsquo;ve had some disagreements with Open Philanthropy around forecasting strategy, worldview diversification, or the wisdom of committing to donate all of Moskovitz&rsquo;s money before he dies, and I haven&rsquo;t felt particularly heard.</p>

<p>Third, I feel that EA leadership uses <a href="https://forum.effectivealtruism.org/posts/T975ydo3mx8onH3iS/ea-is-about-maximization-and-maximization-is-perilous">worries about the dangers of maximization</a> to constrain the rank and file in a hypocritical way. If I want to do something cool and risky on my own, I have to beware of the &ldquo;<a href="https://forum.effectivealtruism.org/topics/unilateralist-s-curse">unilateralist curse</a>&rdquo; and &ldquo;build consensus&rdquo;. But if Open Philanthropy donates <a href="https://www.openphilanthropy.org/grants/openai-general-support/">$30M to OpenAI</a>, <a href="https://forum.effectivealtruism.org/posts/cDdcNzyizzdZD4hbR/critique-of-openphil-s-macroeconomic-policy-advocacy">pulls a not-so-well-understood policy advocacy lever that contributed to the US overshooting inflation in 2021</a>, <a href="https://pitchbook.com/newsletter/ai-specialist-anthropic-grabs-124m">funds Anthropic</a><sup id="fnref:13"><a href="#fn:13" rel="footnote">13</a></sup> while Anthropic&rsquo;s President and the CEO of Open Philanthropy were married, and <a href="https://forum.effectivealtruism.org/posts/trqswoctpQ92tcY2y/criticism-thread-what-things-should-openphil-improve-on">romantic relationships</a> are common between Open Philanthropy officers and grantees, that is ¿an exercise in good judgment? ¿a good ex-ante bet? ¿assortative mating? ¿presumably none of my business?</p>

<p>Fourth, my impression is that the leadership doesn&rsquo;t see itself accountable to the community, but to their understanding of the philosophy and to the funding source. E.g., Holden Karnofsky, the erstwhile head honcho of Open Philanthropy, for a long time didn&rsquo;t <a href="https://twitter.com/NunoSempere/status/1601256399056424960">answer comments on his posts</a>.</p>

<p>Fifth, Open Philanthropy is large enough that it begins to have &ldquo;seeing like a state&rdquo; problems, the problems of bureaucracies. It moves slowly, and seems to have an &ldquo;unfocused glaze&rdquo;. E.g., it took <a href="https://nunosempere.com/blog/2022/06/16/criminal-justice/">two years and an extra $100M</a> to exit the criminal justice cause area. Its forecasting grant-making could have used more small experimentation over large grants to existing organizations. For example, <a href="https://www.astralcodexten.com/p/announcing-forecasting-impact-mini">Scott Alexander&rsquo;s</a> <a href="https://www.astralcodexten.com/p/acx-grants-results">grants</a> seem much more exciting than a <a href="https://www.openphilanthropy.org/grants/metaculus-platform-development/">$8.5</a> <a href="https://www.openphilanthropy.org/grants/metaculus-platform-development-2023/">million</a> to Metaculus, but Open Philanthropy chose the $8.5M to Metaculus and warped the forecasting ecosystem and distribution of talent towards Metaculus-shaped things instead of many small experiments<sup id="fnref:14"><a href="#fn:14" rel="footnote">14</a></sup>.</p>

<p>So overall, my impression is that the leadership of EA holds a &ldquo;leadership without consent&rdquo;, a leadership without much listening and telegraphing one&rsquo;s priorities so that the leaders can coordinate better with those they lead, and incorporate their perspectives and feedback. It falls on the wrong side of the <a href="https://en.wikipedia.org/wiki/Socialist_calculation_debate">socialist calculation debate</a><sup id="fnref:15"><a href="#fn:15" rel="footnote">15</a></sup>, and doesn&rsquo;t compensate enough. And that makes some sense: Open Philanthropy, the main source of funding, is a bureaucracy spun up to spend a billionaire&rsquo;s wealth according to his<sup id="fnref:16"><a href="#fn:16" rel="footnote">16</a></sup> broad, delegated desires. It would then be surprising if they were able to also skillfully steer and command a 10k strong community, and listen and address their worries, absorb their perspectives. But also as a result, I don&rsquo;t feel particularly inclined to take my cues from that machinery.</p>

<h3>6. &hellip;and EA leadership doesn&rsquo;t display a blistering, white-hot competence</h3>

<p>If the EA leadership was, you know, an Arthurian elite which routinely displayed a blistering white hot competence, then I would be more willing to continue pouring my heart and soul into plans of their design in the absence of feedback loops.</p>

<p>But they aren&rsquo;t, so I&rsquo;m not.</p>

<h3>7. Therefore it might make sense to walk away more often</h3>

<p>I see bright-eyed young EAs wanting to roll deeper into the EA rabbit hole and to get employed by EA organizations. They will learn much at first, but later find themselves at the mercy of a machine that can&rsquo;t hear them. Bad move to walk into that without forewarning. I see the EA machine luring brilliant minds that might be better off trying to amass a small fortune through capitalistic entrepreneurship and then deploying that fortune subject to many fewer constraints. I see people with ambitious visions with their wings clipped because they are illegible to grantmakers, and I think, what good knights they would be, if they had a good lord to serve under.</p>

<p>Perhaps it makes sense to instead do something subtly different from EA, to ignore the implicit vibes and expectations of the EA machine. To sometimes take their funding, but to do your own thing and preserve your ability to comfortably leave. To not serve a billionaire&rsquo;s notion of the good within a structure with exceedingly poor feedback loops. To notice that if you could do well inside the EA machine, you might do better outside of it. And sometimes, to simply walk away, to burn the remainder of your youth in the pursuit of making the world a better place, outside of EA.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
You can read a bit more about what I was trying to do <a href="https://forum.effectivealtruism.org/posts/3hH9NRqzGam65mgPG/five-steps-for-quantifying-speculative-interventions">here</a>, and some more reflections <a href="https://nunosempere.com/blog/2023/07/13/melancholy/">here</a>.<a href="#fnref:1" rev="footnote">&#8617;</a></li>
<li id="fn:2">
That is, I think this blog post could plausibly be useful for <em>individual people</em> reading it, not for EA institutionally to address the aspects I discuss. I don&rsquo;t think there is an EA entity with the inclination to digest and address these points.<a href="#fnref:2" rev="footnote">&#8617;</a></li>
<li id="fn:3">
I like bellicose framings, but one could use neutral metaphors instead: &ldquo;&hellip;making mediocre moves, reducing the EA community&rsquo;s ability to do good together&rdquo;, or more flowery ones &ldquo;&hellip;making mediocre moves, reducing the EA&rsquo;s community to flourish and give birth to valuable projects.&rdquo;<a href="#fnref:3" rev="footnote">&#8617;</a></li>
<li id="fn:4">
Incidentally, this is why providing criticism of EA is not a catch-22 where you thereby &ldquo;are&rdquo; &ldquo;an EA&rdquo;, or &ldquo;are doing effective altruism&rdquo;. In particular, you can agree with some of the philosophical attitudes and positions of Effective Altruism, without thereby having to pledge allegiance to the EA machine.<a href="#fnref:4" rev="footnote">&#8617;</a></li>
<li id="fn:5">
E.g., technically, Open Philanthropy is its own thing, and the vehicle for Moskovitz&rsquo;s donations is <a href="https://www.goodventures.org/">Good Ventures</a>. But who cares.<a href="#fnref:5" rev="footnote">&#8617;</a></li>
<li id="fn:6">
For example, per <a href="https://nunosempere.com/gossip/">here</a>, Open Philanthropy donated $450M in 2021. Did other sources of funding cumulative add to more than $45M? My guess is no, and that the distribution of funding is steep. For example, Jan Tallinn donated <a href="https://jaan.online/philanthropy/donations.html#2021">$23M</a> in 2021. So the EA movement wouldn&rsquo;t literally be a <a href="https://en.wikipedia.org/wiki/Monopsony">monopsony</a>, but still, because capital is so concentrated, it seems like capital has much more power compared to labour.<a href="#fnref:6" rev="footnote">&#8617;</a></li>
<li id="fn:7">
There are going to be some free variables, e.g., around what the &ldquo;exchange rates&rdquo; or conversion factors between money, illness and death should be, or around how to value a young person&rsquo;s life vs an older person&rsquo;s. But you can be transparent and predictable about how you will resolve these ambiguities.<a href="#fnref:7" rev="footnote">&#8617;</a></li>
<li id="fn:8">
these are going to be grant officers at Open Philanthropy, but also EA Fund managers, people in charge of hiring decisions at CEA and at large EA organizations, and so on.<a href="#fnref:8" rev="footnote">&#8617;</a></li>
<li id="fn:9">
Readers are also welcome to hypothesize what dynamics arise when trust is scarce. Perhaps <a href="https://en.wikipedia.org/wiki/Peter_principle">promotion to incompetence</a> across the people that are trusted? Or exacerbation of inner circle dynamics?<a href="#fnref:9" rev="footnote">&#8617;</a></li>
<li id="fn:10">
You can solve this problem by having grant-makers be anonymous. Here is a robinhansonian design: have a cohort of anonymous regrantors and allow members of the public to make $20k bets at 1:2 odds on whether any one particular person is a grant-maker. This ensures that your regrantors will remain anonymous. Anonymous philanthropy has precedents, see e.g., <a href="https://forum.nunosempere.com/posts/yS3qRbHzzWxjR2Ehp/chuck-feeney-1931-2023">here</a>.<a href="#fnref:10" rev="footnote">&#8617;</a></li>
<li id="fn:11">
Doesn&rsquo;t seem like a great <a href="https://en.wikipedia.org/wiki/Attachment_theory">attachment theory</a> setup.<a href="#fnref:11" rev="footnote">&#8617;</a></li>
<li id="fn:12">
Incidentally, having romantic relationships with Open Philanthropy employees increases <a href="https://forum.effectivealtruism.org/posts/trqswoctpQ92tcY2y/criticism-thread-what-things-should-openphil-improve-on">access</a> to that coterie. That is, I suspect that having a close relationship with Open Phil people <a href="https://www.lesswrong.com/tag/privileging-the-hypothesis">privileges the hypothesis</a> that your grant is worth evaluation.<a href="#fnref:12" rev="footnote">&#8617;</a></li>
<li id="fn:13">
For some confirmatory evidence, note that Luke Muehlhauser, an Open Philanthropy grantmaker, is a <a href="https://www.openphilanthropy.org/research/12-tentative-ideas-for-us-ai-policy/#f+13628+2+4">board member at Anthropic</a>.<a href="#fnref:13" rev="footnote">&#8617;</a></li>
<li id="fn:14">
I find it interesting that when he left Open Philanthropy to start the FTX Future Fund, Nick Beckstead (with others) designed it to look completely different than the Open Philanthropy model: trusting independent and eclectic expert regrantors to make grants according to their judgment, evaluated on their performance, rather than hierarchies of grantmakers each restricted to a cause or sub-cause.<a href="#fnref:14" rev="footnote">&#8617;</a></li>
<li id="fn:15">
See <a href="https://www.libertarianism.org/topics/socialist-calculation-debate">here</a> for a more libertarian perspective which disagrees in emphasis with the Wikipedia page.<a href="#fnref:15" rev="footnote">&#8617;</a></li>
<li id="fn:16">
and his wife&rsquo;s<a href="#fnref:16" rev="footnote">&#8617;</a></li>
</ol>
</div>

<p>
  <section id='isso-thread'>
  <noscript>javascript needs to be activated to view comments.</noscript>
  </section>
</p>
